{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Environment Setup & Dependencies\n",
        "In this section, we install the necessary libraries for efficient LLM fine-tuning.\n",
        "\n",
        "\n",
        "\n",
        "*   bitsandbytes: Enables 4-bit quantization, allowing us to load massive models (like Llama-3 8B) into GPU memory by compressing weights.\n",
        "*   peft (Parameter-Efficient Fine-Tuning): Instead of training the entire model (which is expensive), we use LoRA (Low-Rank Adaptation) to train only a tiny fraction of parameters.\n",
        "*   trl (Transformer Reinforcement Learning): Provides the SFTTrainer, a wrapper optimized for Supervised Fine-Tuning of LLMs.\n",
        "*   accelerate: Handles hardware optimization, distributing computations efficiently across the GPU.\n",
        "\n",
        "# Mathematical Foundation: Quantization (NF4)\n",
        "We use bitsandbytes to load the model in 4-bit NormalFloat (NF4). Unlike standard linear quantization, NF4 is information-theoretically optimal for weights that follow a normal distribution (which most LLM weights do).The quantization maps a high-precision weight $W$ to a discrete 4-bit value $q$. The quantization levels $q_i$ are determined by the quantiles of the Standard Normal Distribution $N(0,1)$:$$q_i = Q^{-1}\\left(\\frac{i}{2^k + 1}\\right)$$Where:$Q^{-1}$ is the inverse Cumulative Distribution Function (CDF) of the Normal distribution.$k$ is the number of bits (here $k=4$, yielding 16 distinct levels).This ensures that the limited 4-bit \"bins\" are concentrated where the majority of the weight values actually exist (near zero), minimizing the Quantization Error\n",
        "\n",
        "($Error = ||W - \\text{dequant}(W_{q})||^2$).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XBWrJ3GbySkK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goAC7G0ayH9a"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
        "!pip install -q datasets einops wandb bitsandbytes matplotlib\n",
        "\n",
        "# Check GPU status\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    # A100 Optimization Check\n",
        "    if torch.cuda.get_device_capability()[0] >= 8:\n",
        "        print(\" A100/H100/RTX3090+ Detected: Flash Attention 2 & BFloat16 enabled.\")\n",
        "    else:\n",
        "        print(\" Legacy GPU Detected: Fallback to FP16.\")\n",
        "else:\n",
        "    print(\" No GPU detected.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Configuration & Architecture\n",
        "We use a Configuration Class (TrainingConfig). This acts as a central control panel for hyperparameters.\n",
        "\n",
        "Key Decisions for A100 vs. T4:\n",
        "\n",
        "* Precision (bf16 vs fp16): On an A100, we strictly use bf16 (Brain Float 16). It has the same dynamic range as Float32, preventing \"NaN\" (Not a Number) errors during training without needing a gradient scaler. On a T4, we would rely on fp16.\n",
        "\n",
        "* Attn Implementation: We enable flash_attention_2. This is a hardware-aware algorithm that speeds up the Attention mechanism by ~3x on A100 GPUs.\n",
        "\n",
        "# Mathematical Foundation: Precision & Dynamic Range\n",
        "\n",
        "We select BFloat16 (bf16) over Float16 (fp16) for the A100.Float32 (Standard): 1 Sign bit, 8 Exponent bits, 23 Mantissa bits.Float16 (Legacy): 1 Sign bit, 5 Exponent bits, 10 Mantissa bits. Narrow dynamic range leads to underflow (gradients becoming 0).BFloat16 (Brain Float): 1 Sign bit, 8 Exponent bits, 7 Mantissa bits.$$\\text{Range}(BF16) \\approx \\text{Range}(FP32)$$Because BFloat16 preserves the 8-bit exponent of FP32, it prevents numerical instability during training without requiring specific gradient scaling techniques."
      ],
      "metadata": {
        "id": "ElYRAffezL5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import time\n",
        "import psutil\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# Centralized Configuration\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    model_name: str = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "    dataset_name: str = \"timdettmers/openassistant-guanaco\"\n",
        "    new_model_name: str = \"Llama-3-8B-Instruct-Guanaco-Adapter\"\n",
        "\n",
        "    # QLoRA Parameters\n",
        "    lora_r: int = 64              # Rank: Higher = more parameters to train, smarter but slower\n",
        "    lora_alpha: int = 16          # Scaling factor\n",
        "    lora_dropout: float = 0.05\n",
        "\n",
        "    # Training Parameters\n",
        "    batch_size: int = 1            # Per device batch size\n",
        "    grad_accum_steps: int = 4      # Accumulate gradients to simulate larger batch size\n",
        "    num_epochs: int = 1            # Total epochs (for 1 epoch ~ 2 Hrs)\n",
        "    learning_rate: float = 2e-4\n",
        "    max_seq_length: int = 2048     # Llama-3 supports up to 8k, but 2048 is standard for fine-tuning\n",
        "\n",
        "    # Hardware Optimization (A100 Specifics)\n",
        "    use_4bit: bool = True          # Activate QLoRA\n",
        "    bnb_4bit_compute_dtype: str = \"bfloat16\" # computation type\n",
        "    use_nested_quant: bool = False\n",
        "\n",
        "    # Output\n",
        "    output_dir: str = \"./results\"\n",
        "\n",
        "config = TrainingConfig()"
      ],
      "metadata": {
        "id": "0OQWgshwzFzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. The LLMFineTuner Engine\n",
        "We encapsulate the entire lifecycle in a Python Class. This abstraction allows us to initialize, train, and inference with single commands.\n",
        "\n",
        "Concepts Used:\n",
        "\n",
        "* Gradient Checkpointing: This technique saves VRAM by not storing all intermediate activations during the forward pass. Instead, it recomputes them during the backward pass. It trades a small amount of compute for massive memory savings.\n",
        "\n",
        "* QLoRA (Quantized LoRA): We load the base model in 4-bit (lossy compression) but perform the training calculations in 16-bit. This allows us to finetune a 16GB model on a card with less memory, though A100 handles this easily.\n",
        "\n",
        "* Tokenizer Padding: Llama-3 does not have a native PAD token. We set pad_token = eos_token to ensure batches are processed correctly.\n",
        "\n",
        "# Mathematical Foundation: Low-Rank Adaptation (LoRA)\n",
        "\n",
        "Fine-tuning a full model requires updating all parameters $W$. LoRA freezes the pre-trained weights $W_0$ and injects trainable rank decomposition matrices $A$ and $B$.For a layer with input $x$, the forward pass becomes:$$h = W_0 x + \\Delta W x = W_0 x + \\frac{\\alpha}{r} (B A) x$$Where:$W_0 \\in \\mathbb{R}^{d \\times k}$: Frozen pre-trained weights.$B \\in \\mathbb{R}^{d \\times r}$ and $A \\in \\mathbb{R}^{r \\times k}$: Trainable adapter matrices.$r \\ll \\min(d, k)$: The rank (e.g., $r=64$).$\\alpha$: A scaling constant to control signal strength.This reduces the number of trainable parameters by a factor of roughly $r/d$. For Llama-3, this reduces trainable parameters from 8 Billion to roughly 160 Million (~2%), massively reducing VRAM requirements while maintaining performance.Optimization: Flash Attention 2Standard attention computes an $N \\times N$ matrix ($S = QK^T$), leading to $O(N^2)$ memory complexity. Flash Attention tiles the computation to keep data in the GPU's fast SRAM, avoiding slow HBM (High Bandwidth Memory) reads/writes.$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_{head}}}\\right)V$$While the formula is identical, Flash Attention computes this block-wise, achieving near-linear IO complexity with respect to sequence length."
      ],
      "metadata": {
        "id": "lGNWVDpd0FCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMFineTuner:\n",
        "    def __init__(self, config: TrainingConfig):\n",
        "        self.config = config\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.trainer = None\n",
        "        self.device_map = \"auto\"\n",
        "\n",
        "    def load_model_and_tokenizer(self):\n",
        "        print(f\" Loading Base Model: {self.config.model_name}...\")\n",
        "\n",
        "        # 1. BitsAndBytes Config (Quantization)\n",
        "        compute_dtype = getattr(torch, self.config.bnb_4bit_compute_dtype)\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=self.config.use_4bit,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=compute_dtype,\n",
        "            bnb_4bit_use_double_quant=self.config.use_nested_quant,\n",
        "        )\n",
        "\n",
        "        # 2. Load Base Model\n",
        "        # A100 OPTIMIZATION: attn_implementation=\"flash_attention_2\"\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.config.model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=self.device_map,\n",
        "            attn_implementation=\"flash_attention_2\"\n",
        "        )\n",
        "\n",
        "        # Disable cache for training (reduces VRAM)\n",
        "        self.model.config.use_cache = False\n",
        "        self.model.config.pretraining_tp = 1\n",
        "\n",
        "        # 3. Load Tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name, trust_remote_code=True)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.tokenizer.padding_side = \"right\"\n",
        "\n",
        "        print(\" Model and Tokenizer Loaded Successfully.\")\n",
        "\n",
        "    def configure_lora(self):\n",
        "        # Target modules for Llama-3 (All linear layers for best performance)\n",
        "        peft_config = LoraConfig(\n",
        "            lora_alpha=self.config.lora_alpha,\n",
        "            lora_dropout=self.config.lora_dropout,\n",
        "            r=self.config.lora_r,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\",\n",
        "            target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
        "        )\n",
        "        return peft_config\n",
        "\n",
        "    def train(self):\n",
        "        if not self.model:\n",
        "            self.load_model_and_tokenizer()\n",
        "\n",
        "        dataset = load_dataset(self.config.dataset_name, split=\"train\")\n",
        "        peft_config = self.configure_lora()\n",
        "\n",
        "        training_arguments = SFTConfig(\n",
        "            output_dir=self.config.output_dir,\n",
        "            num_train_epochs=self.config.num_train_epochs,\n",
        "            per_device_train_batch_size=self.config.batch_size,\n",
        "            gradient_accumulation_steps=self.config.grad_accum_steps,\n",
        "            optim=\"paged_adamw_32bit\",\n",
        "            save_steps=25,\n",
        "            save_total_limit=4, # will only save newest 4 checkpoints and delete the older ones to save disk space\n",
        "            logging_steps=25,\n",
        "            learning_rate=self.config.learning_rate,\n",
        "            weight_decay=0.001,\n",
        "            fp16=False,\n",
        "            bf16=True, # A100 supports BFloat16 natively\n",
        "            max_grad_norm=0.3,\n",
        "            max_seq_length=self.config.max_seq_length,\n",
        "            warmup_ratio=0.03,\n",
        "            group_by_length=True,\n",
        "            lr_scheduler_type=\"cosine\",\n",
        "            report_to=\"tensorboard\",\n",
        "            dataset_text_field=\"text\"\n",
        "        )\n",
        "\n",
        "        self.trainer = SFTTrainer(\n",
        "            model=self.model,\n",
        "            train_dataset=dataset,\n",
        "            peft_config=peft_config,\n",
        "            processing_class=self.tokenizer,\n",
        "            args=training_arguments,\n",
        "        )\n",
        "\n",
        "        # Fix for norm layers in quantized models\n",
        "        for name, module in self.trainer.model.named_modules():\n",
        "             if \"norm\" in name:\n",
        "                 module = module.to(torch.float32)\n",
        "\n",
        "        print(\" Starting Training...\")\n",
        "        start_time = time.time()\n",
        "        self.trainer.train()\n",
        "        end_time = time.time()\n",
        "\n",
        "        training_time = end_time - start_time\n",
        "        print(f\" Training Complete in {training_time/60:.2f} minutes.\")\n",
        "\n",
        "        # Save Model\n",
        "        self.trainer.model.save_pretrained(self.config.new_model_name)\n",
        "        self.tokenizer.save_pretrained(self.config.new_model_name)\n",
        "\n",
        "        return self.trainer\n",
        "\n",
        "    def generate_response(self, prompt, max_new_tokens=100):\n",
        "        # Simple inference method\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "qPX4EsQIzF5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Execution Pipeline\n",
        "We now instantiate our LLMFineTuner and start the process.\n",
        "\n",
        "Resource Monitoring: We use psutil and PyTorch commands to track VRAM usage. Note on Epochs: We set num_epochs=1. For large datasets like Guanaco (9k samples), 1 epoch is often sufficient for the model to learn the style without overfitting (forgetting its general knowledge).\n",
        "\n",
        "# Mathematical Foundation: Gradient Accumulation\n",
        "\n",
        "Since LLMs are large, we cannot fit a large batch size $B$ into memory. We simulate a large batch size using Gradient Accumulation. We compute gradients for smaller micro-batches and sum them up before performing a weight update.$$B_{effective} = B_{micro} \\times N_{accum} \\times N_{GPUs}$$The weight update rule (SGD example) happens only after $N_{accum}$ steps:$$W_{t+1} = W_t - \\eta \\frac{1}{B_{effective}} \\sum_{i=1}^{B_{effective}} \\nabla \\mathcal{L}(x_i, y_i)$$This allows us to mathematically simulate a large batch size (stable training) while physically only holding a small batch in VRAM."
      ],
      "metadata": {
        "id": "eA_9JAmr0msY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to HuggingFace (Required for Llama-3)\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "\n",
        "# Initialize the Engine\n",
        "tuner = LLMFineTuner(config)\n",
        "\n",
        "# Track Memory Before\n",
        "torch.cuda.empty_cache()\n",
        "gpu_stats_before = torch.cuda.memory_allocated() / 1024**3\n",
        "print(f\"Initial GPU Memory: {gpu_stats_before:.2f} GB\")\n",
        "\n",
        "# Start Training\n",
        "trainer_instance = tuner.train()\n",
        "\n",
        "# Track Memory After\n",
        "gpu_stats_after = torch.cuda.memory_allocated() / 1024**3\n",
        "print(f\"Peak GPU Memory: {gpu_stats_after:.2f} GB\")"
      ],
      "metadata": {
        "id": "Ts5fLhpczF7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Training Metrics Visualization\n",
        "Analyzing the Loss Curve is crucial.\n",
        "\n",
        "* Loss: Represents the error. It should decrease over time.\n",
        "\n",
        "* Overfitting: If Training Loss goes down but Validation Loss goes up (or stays flat), the model is memorizing data rather than learning patterns.\n",
        "\n",
        "* We extract the logs from the trainer history to visualize this.\n",
        "\n",
        "# Mathematical Foundation: Cross-Entropy Loss\n",
        "\n",
        "The model is trained to predict the next token $u_t$ given the history $u_{<t}$. We minimize the negative log-likelihood over the dataset $D$:$$\\mathcal{loss}(\\theta) = -\\sum_{(u) \\in D} \\sum_{t=1}^{T} \\log P(u_t | u_{<t}; \\theta)$$Loss Decrease: Indicates the model is assigning higher probability to the correct next tokens.Perplexity (PPL): A common metric derived from loss: $PPL = e^{\\mathcal{loss}}$. Lower is better."
      ],
      "metadata": {
        "id": "CAnkr7hd00Om"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(trainer):\n",
        "    log_history = trainer.state.log_history\n",
        "\n",
        "    steps = []\n",
        "    losses = []\n",
        "\n",
        "    for log in log_history:\n",
        "        if \"loss\" in log:\n",
        "            steps.append(log[\"step\"])\n",
        "            losses.append(log[\"loss\"])\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(steps, losses, label=\"Training Loss\", color=\"blue\")\n",
        "    plt.title(\"Model Training Loss Curve\")\n",
        "    plt.xlabel(\"Training Steps\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Visualize\n",
        "plot_training_history(trainer_instance)"
      ],
      "metadata": {
        "id": "l4wXKCiizF-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Inference (Testing the Model)\n",
        "Now that the model is trained, we must test it. Note that we are using the Adapter (LoRA weights) combined with the Base Model.\n",
        "\n",
        "* Temperature: Controls randomness. Low (0.1) is factual/robotic. High (1.0) is creative/chaotic. We use 0.7 for a balance."
      ],
      "metadata": {
        "id": "gk1LsRgU0_vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean memory for inference\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "user_prompt = \"Human: What are the main differences between Python and C++? Assistant:\"\n",
        "\n",
        "print(\"generating response...\")\n",
        "response = tuner.generate_response(user_prompt)\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(\"FINAL GENERATED OUTPUT:\")\n",
        "print(\"-\" * 50)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "kzm2a0t9zGCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you use this on a T4 GPU, make these changes in TrainingConfig:\n",
        "\n",
        "* bf16 = False, fp16 = True (T4 doesn't support BFloat16).\n",
        "\n",
        "* bnb_4bit_compute_dtype = \"float16\".\n",
        "\n",
        "* Remove attn_implementation=\"flash_attention_2\" (T4 doesn't support Flash Attention 2)."
      ],
      "metadata": {
        "id": "GLrUSdTw1M-H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F5AxMIpvzGTf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}