{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
    "!pip install -q datasets einops wandb\n",
    "!pip install -q bitsandbytes"
   ],
   "metadata": {
    "id": "9Yl6G2UpFqYC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install trl"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2rfJIDR8F6ZK",
    "outputId": "1bc6cd41-3722-40ea-98df-16c2c73c5e68"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting trl\n",
      "  Downloading trl-0.28.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl) (1.12.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
      "Requirement already satisfied: packaging>20.0 in /usr/local/lib/python3.12/dist-packages (from trl) (26.0)\n",
      "Requirement already satisfied: transformers>=4.56.2 in /usr/local/lib/python3.12/dist-packages (from trl) (5.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.0.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.9.0+cu128)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (1.4.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (4.67.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.2->trl) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.2->trl) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.2->trl) (0.21.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2026.1.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers>=4.56.2->trl) (8.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.12.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (0.16.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.3)\n",
      "Downloading trl-0.28.0-py3-none-any.whl (540 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m540.5/540.5 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: trl\n",
      "Successfully installed trl-0.28.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "pip install -U bitsandbytes>=0.46.1"
   ],
   "metadata": {
    "id": "RIMxyspcR9VE"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SHDWHyNGFS2h"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "logging.set_verbosity_info()\n",
    "logger = logging.get_logger(\"transformers\")"
   ],
   "metadata": {
    "id": "dv9qrD-OGi4h"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x9W9KmVlFy8N",
    "outputId": "d5e6f426-0f49-4ce2-fe58-8d54d78579e7"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA A100-SXM4-40GB\n",
      "GPU Memory: 42.41 GB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "c2c405b06263475f8d7a2806d709d192",
      "8da4039649cb408d96e27aa73046c036",
      "bab8a10ae6624e22a4fdcc594e0f8634",
      "d2134d5078bd424e836a03d4293572f2",
      "b6013d43b5724b53b0666249c6773520",
      "e11368165dbe4cd18b7139fa9f135faa",
      "a29940b2d08946e499170903b09fad75",
      "7afe4a451b894a8d92b6f17114708bdb",
      "f9c6b78170504b5b977be301678c6d61",
      "b62066c726d8409dacc0bbc8996106c0",
      "c8050422c4dc49c091ff476a9c086b5e",
      "6b9ecc07332445798cee88b1ceb9aa2b",
      "5e8c00284fe5476ba4484fa502bab1c2",
      "57752c23edd84e7888c04d3081310790",
      "dffcf894d009461d9f758a5540ba135f",
      "868138de0c1a4cff8f24858d19d49b41",
      "d5fc375208c1495bb3e394f5df12b3ce",
      "e4b2dd3c34a84f93b9865b111995e66b",
      "daade4c3baff454f8706282e35df71cc",
      "4b836c2565d84f3ba7a617fdefccdf3e"
     ]
    },
    "id": "3ahnRgWHIB-A",
    "outputId": "14c78352-ef42-4c00-f3e6-e40fa545316b"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2c405b06263475f8d7a2806d709d192"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_name = \"timdettmers/openassistant-guanaco\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "print(f\"Guanaco Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset size: {len(dataset):,} examples\")\n",
    "print(f\"\\nDataset features:\")\n",
    "print(dataset.features)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316,
     "referenced_widgets": [
      "ad27a1b4e9de49e2956d792de05cf259",
      "9cfaaeca594e46d2a82ad893b20c4cdf",
      "07c54b5c2a914d2698b1b8400293eec7",
      "b48b17ed67ea41e48c3e8a2599bd82fd",
      "4593cf890bf84eac92d41208bcc04eff",
      "aaedbd51488a4cb19f0cac3830ce78f3",
      "42391d58bacc4dfea69dbbff97983496",
      "16ed7feddc754c99a157d0d6c8d76dbb",
      "17cb5599c4f24c8882f0da7bddcddd07",
      "90dc36af355a4c2c88d55eaf3860e777",
      "e14d534f94d644d1af5e4c7f7e964fc0",
      "6568b96088fd455392058ea2b2236f66",
      "2846ff83b20348db9e212e1effc94fdd",
      "f4d29dc70fe740318022f00f2b01802e",
      "05bd3de7b9be42e1b6577f0fd1e68f84",
      "05a3c8a12e474cef8a659e76bf3892db",
      "dc6f99e9f3034a1782b7c9d08f4e64dc",
      "97fdfc530b9141a8823c2b7bc18a0960",
      "bb29c1a6355b4c87aa70893e4072bd88",
      "a10e94525d6841bc8d1aea2dcad54300",
      "f8ef9184b9de48bdb9e4f2bcb055b6b3",
      "9494a4984c7942c8b1eccef6645bbbcc",
      "1e88323040e44d99a4aba26cbdf7f574",
      "b104a696e0a846d6ae256dff54eedfd7",
      "8f3ff81d4c8c443b82eea557bbe2807d",
      "c1958f08ffde4de196181b24c3c3f38b",
      "7938cd2d97f54c328ce70c4e2c90a7f9",
      "fe046e4457e74edda14f460ae758d80d",
      "619e7ca6e06b4d03a87c27660cc4ba7d",
      "359cecbdf5964e3e82c032bd6262f537",
      "9004b0648bdd4a47acd6031d1ff37f31",
      "c741b82fd36946ed8e72177baca0d2d3",
      "548e925c7fce41bb9c44cb4efb30b367",
      "ade755e904a748a3800e28d7597b23cb",
      "d59c48c58959439483332d617c716d0f",
      "c4c22abd68b84cb88efe967fa63a5bbc",
      "a310e38d852f4d6b87d017e80eaf0aa3",
      "067f6dce161444a997d5c6ec02525f36",
      "4ca8ecfb4d6f4cefa4894df85ed4234e",
      "eef8a8789c89407d85cb30006b8234c2",
      "dd14a7aef74d4431836162ae6b1f1fe7",
      "86a18b1c0d85490aa8b48397399f61c8",
      "3e8f8071bc1f406e92405abc713cc9fd",
      "fe51b968e37d45f9afd9d338d4439767",
      "136a2a91d13c42de98fe51042b9ceda2",
      "b75ee7a1c8fb41f8afbc3c3d57df5956",
      "61996297887b469abf7b14d5f86a6922",
      "049845b63f8245eab44d6d852c283966",
      "3e2a6eee54b3410b99a9d3736c812711",
      "fe9fe4aef05943d79342541455c64c3f",
      "2ae627d278a34601b0a2d819365a9746",
      "6fdde65448234509b61d04af2a4997de",
      "4b2dfcc576c84246acb7665f67678b46",
      "fe20bc01e68144c58213e54ddd7c8928",
      "c9d7924589c54cde805d0239db530a6c"
     ]
    },
    "id": "uuOd0hd-GBRV",
    "outputId": "1cdcc78e-2e99-46b1-862a-e5e01bce2b40"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/395 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad27a1b4e9de49e2956d792de05cf259"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "openassistant_best_replies_train.jsonl:   0%|          | 0.00/20.9M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6568b96088fd455392058ea2b2236f66"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "openassistant_best_replies_eval.jsonl: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e88323040e44d99a4aba26cbdf7f574"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ade755e904a748a3800e28d7597b23cb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/518 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "136a2a91d13c42de98fe51042b9ceda2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Guanaco Dataset loaded successfully!\n",
      "\n",
      "Dataset size: 9,846 examples\n",
      "\n",
      "Dataset features:\n",
      "{'text': Value('string')}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"------------Sample1-------------\")\n",
    "print()\n",
    "i = 0\n",
    "print(f\"Index: {i}\")\n",
    "print(f\"Text: {dataset[i][\"text\"]}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QPXUgH_xH2CG",
    "outputId": "aac2c1da-1e4a-4675-f3b5-45cc23e9ed8c"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------Sample1-------------\n",
      "\n",
      "Index: 0\n",
      "Text: ### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\n",
      "\n",
      "Recent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\n",
      "\n",
      "Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\n",
      "\n",
      "References:\n",
      "Bivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.### Human: Now explain it to a dog\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "load_in_4bit = True\n",
    "bnb_4bit_compute_dtype = torch.float16\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False"
   ],
   "metadata": {
    "id": "RvIpxbeaPvMj"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant\n",
    ")"
   ],
   "metadata": {
    "id": "83s9AhrjMrVd"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "print('---------------------------------------')\n",
    "print(f\"Loading {model_name}, Please Wait ...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(\"\\n Model loaded successfully!\")\n",
    "print(f\"Model: {model_name}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "104639301eef496b88b7b775b8e1740f",
      "c9078279e44f449ba7309b1c2c8c4710",
      "88f64363d1904579957b830095bf03c0",
      "5c2ebfa9aedb4bcdac0e31c56379c64a",
      "7ec5ab741e234dfe865a767009742d94",
      "6d28f1741d954f0b95fa54c33c85b617",
      "cfb723fccf4d49d188a4281995a554ed",
      "b9fa1552b55946d6ad2b09c4bf45bb9b",
      "e76e7662435e430b8d9ea99fdc44c20c",
      "42ad191967084773850cb6a49397f989",
      "4c6d9d19086f4f55b3ef2e8b241d45e7",
      "383cbd035f9f4f9eb496322b66c67906",
      "c1de927c4c6c46e8ab9f838a5e77d777",
      "5bb72ce48d6c4e29a97efeabd0dfe6eb",
      "87c1a718075f425299277743f7199514",
      "b9370d3c92684b1f89f5a8b65e182223",
      "23fabe0e62ef4a2c81d1688fa34fee35",
      "78c7530abb5d41f9b2b6078ff6025e25",
      "9804584838c84db6a5c15f50b3755917",
      "5e93832c6e1b4eedb8678e9031790ca6",
      "5c653155875a4f7aa7bf39406233c114",
      "887d340853e741feb0f3fd63fd0958ab",
      "a726d1f8c3094e4cad8aa63026f4f84a",
      "c387f5a00332487fa1a3ffcca78186de",
      "037cf4e40e51450b863fbb15e7e9f26a",
      "bbc648bc926c442188fd16778a19a449",
      "bfdf18ad78c041e29b0f15027bdca9dc",
      "4a1027a5d1b440d9a2fc060113ae755d",
      "b50d21b64d55481bb8cfb15739c0e410",
      "6302f7e8f16448049bb3e8b7f8aa2daf",
      "89e1c38c54dc4ad880a5838c770eeecd",
      "5f08b61e63f146e4a8ed63f9a94ae9e8",
      "5b525a5188764668a7e7f0c04d16d52d",
      "8937837c8ebb4417963de84733f38c84",
      "82287bbb925e4a36971d80b6aedcb39a",
      "7a38c5fd9dad41428ee0103058283e64",
      "c5f85edfc5554081bad4df806e4a616d",
      "918bb46a14a44d479cd4c611e93bfa43",
      "2cd02936d2614621a87fafb8f2a77315",
      "e4849dcc8a824e169c16c26a93dcdf82",
      "8daba36c45d147229d23bca0041f5c9a",
      "a62588c632254782a94fdaa318f0c12c",
      "b48f81fab4d1481cb407c377fcb7b540",
      "9102f42fa48a44dd9849ce1367873646",
      "57111a3dbb754cd09aad34e0645fe24c",
      "d44ff037f8854a1d8f8b2f0118b49b55",
      "7d77ebd47ba04627ac2cae96959ad61c",
      "c7f6e00ca91e4ef482e173849c0b9610",
      "3aa1866662e14dc49eb6eeac633a33c6",
      "ff57ac11d5a0424082d0748f52039138",
      "db9fb0ac1e134e21b18488c08aee3dd3",
      "5b10320c1117422a8db4c8816ef710a8",
      "07528ec441394d8f9891345b96f6a99f",
      "e818b1dbec5f41db8e8c71a0a9257acc",
      "9710892053dc4e78b5839dae55e67c9d",
      "a14a5a12bb1d441d9cab23fd60d06403",
      "d3681a8ad81841c0ac9c88476c68b216",
      "8ac19a09609b4e32a60c5b4a1f8ca555",
      "97c94a9a7e6049d1929eeff0c48f1354",
      "eeec400c2282419181b47192fd2be661",
      "4d06d6b5dca34fbdb1bf20bb5b740ce8",
      "166ead6461e44bbf8544aa83a4e11bb4",
      "241086a774ce4e639f2b95b1698dd9f6",
      "88903f2b6e7b49f4a6e5a244c0f1ecf6",
      "232a9caa6cda4b70bc2f86619185fc4c",
      "f8ebb8f442c14bb68f42f7e485800f43"
     ]
    },
    "id": "ro0UT-_iIvtq",
    "outputId": "65a45922-40a8-4c0f-b3e6-f2290efec43a"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---------------------------------------\n",
      "Loading meta-llama/Meta-Llama-3-8B-Instruct, Please Wait ...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "104639301eef496b88b7b775b8e1740f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "383cbd035f9f4f9eb496322b66c67906"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/model.safetensors.index.json\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a726d1f8c3094e4cad8aa63026f4f84a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8937837c8ebb4417963de84733f38c84"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"use_cache\": true\n",
      "}\n",
      "\n",
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57111a3dbb754cd09aad34e0645fe24c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a14a5a12bb1d441d9cab23fd60d06403"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 4096,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Model loaded successfully!\n",
      "Model: meta-llama/Meta-Llama-3-8B-Instruct\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count total and trainable parameters\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "total, trainable = count_parameters(model)\n",
    "\n",
    "print(\"------ Model Information ------\")\n",
    "print(f\"Total parameters: {total:,} ({total/1e9:.2f}B)\")\n",
    "print(f\"Trainable parameters (before LoRA): {trainable:,}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zcGXLM_RJwcU",
    "outputId": "4a60fe3c-6259-4cf2-8529-e8104c33a57f"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------ Model Information ------\n",
      "Total parameters: 4,540,600,320 (4.54B)\n",
      "Trainable parameters (before LoRA): 1,050,939,392\n",
      "\n",
      "Model architecture:\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Loading Tokenizer\")\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"Tokenizer loaded successfully\")\n",
    "print(f\"\\nVocabulary Size: {len(tokenizer)}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token}, ID: {tokenizer.eos_token_id}\")\n",
    "print(f\"PAD token: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 824,
     "referenced_widgets": [
      "3caf1e649dce4094a92daab59b988fd9",
      "43e39b70c5fc4314b6cf2b52fd4f72be",
      "6227b3044b274fd6bc85cdf91e9537f1",
      "41f064f7224d40098a981a265baa246a",
      "600ae064e6694bfa9ed256eed0a461f6",
      "41fe68c6356a460796f7f4a34f2ff496",
      "19fabab17c824f5fa7201d1da5993ea6",
      "d755b0571661402b8703442a097daffb",
      "330d512e040944ab8910b607dbc99464",
      "5ebbdda06013482da643d1b0fcf3b67c",
      "e7379afbf4974a159c05b67d8107dbe8",
      "72c42dbfeb014f29a53eb23cb197d4a7",
      "150e9828ee0b41d58b9306670ccf4029",
      "e69e3f7bc97141a6b7812efb43369d08",
      "9375b429f6614a8e9ae9cb1733ad935a",
      "5b1ec1061d9745db908af471754ed6a4",
      "360e48e0bbb9479eb597b451141757c6",
      "9c2b6a2bb8ea49a0bc3573996c453a70",
      "e4ac4860760f482293c6e6edc3f4c5fe",
      "8dd523b9044f4b71a6f3d02d8d9947a8",
      "280761abd91d4a8585ea6521f48680f2",
      "dd7d0be5633d407298d4f3c462c41348",
      "8d67975b13aa44adb986add1f9f4a0c5",
      "84899cea44db45d98f98b5745075d0aa",
      "4ccabeba4fc64329a0a2d120dae32511",
      "834c60a631c04c02b4267888f7497178",
      "87c9085d2c764b90a699b4539a16cd7e",
      "9305a7c8cd40487dbfbe6c19857ae7ed",
      "4de48214112d45e69af21ff89ef44676",
      "34ff98ddf8b3493396adc190f9212393",
      "880d9a39295c4153ba6b4409f336e3b5",
      "35a3f6da3d114f3da49f9321fac47605",
      "229ae0e14b184f93b120d5b07a2e0d59"
     ]
    },
    "id": "d7UCxhEqTChR",
    "outputId": "746ed9dd-bf5d-426b-9ec2-412381dc9129"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Tokenizer\n",
      "---------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3caf1e649dce4094a92daab59b988fd9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72c42dbfeb014f29a53eb23cb197d4a7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d67975b13aa44adb986add1f9f4a0c5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tokenizer loaded successfully\n",
      "\n",
      "Vocabulary Size: 128256\n",
      "EOS token: <|eot_id|>, ID: 128009\n",
      "PAD token: <|eot_id|>, ID: 128009\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Test the tokenizer\n",
    "test_text = \"Hello! How are you doing, Could you help me with some task?\"\n",
    "tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Original text: {test_text}\")\n",
    "print(f\"\\nTokenized IDs: {tokens['input_ids'][0].tolist()}\")\n",
    "print(f\"Number of tokens: {len(tokens['input_ids'][0])}\")\n",
    "print(f\"\\nDecoded back: {tokenizer.decode(tokens['input_ids'][0])}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tfp-IXuPaQ8J",
    "outputId": "ff472b88-d918-4b6c-ad03-8b947b8e1b38"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original text: Hello! How are you doing, Could you help me with some task?\n",
      "\n",
      "Tokenized IDs: [128000, 9906, 0, 2650, 527, 499, 3815, 11, 16910, 499, 1520, 757, 449, 1063, 3465, 30]\n",
      "Number of tokens: 16\n",
      "\n",
      "Decoded back: <|begin_of_text|>Hello! How are you doing, Could you help me with some task?\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "lora_r = 64\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "target_modules = [\n",
    "    'q_proj',\n",
    "    'k_proj',\n",
    "    'v_proj',\n",
    "    'o_proj',\n",
    "    'gate_proj',\n",
    "    'up_proj',\n",
    "    'down_proj'\n",
    "    ]\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules\n",
    ")\n",
    "\n",
    "# print lora config details\n",
    "print(\" LoRA configuration created:\")\n",
    "print(f\"  - Rank (r): {lora_r}\")\n",
    "print(f\"  - Alpha: {lora_alpha}\")\n",
    "print(f\"  - Dropout: {lora_dropout}\")\n",
    "print(f\"  - Target modules: {peft_config.target_modules}\")\n",
    "print(f\"  - Task type: {peft_config.task_type}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fzda5Pb4bEzv",
    "outputId": "086d1631-af32-4bbb-b8b1-876bec8ec434"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " LoRA configuration created:\n",
      "  - Rank (r): 64\n",
      "  - Alpha: 16\n",
      "  - Dropout: 0.05\n",
      "  - Target modules: {'q_proj', 'gate_proj', 'v_proj', 'o_proj', 'down_proj', 'k_proj', 'up_proj'}\n",
      "  - Task type: CAUSAL_LM\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Training Params\n",
    "output_dir = \"./results\"\n",
    "num_train_epochs = 1\n",
    "fp16 = False\n",
    "bf16 = True\n",
    "per_device_train_batch_size = 1\n",
    "gradient_accumulation_steps = 4\n",
    "gradient_checkpointing = True\n",
    "max_grad_norm = 0.3\n",
    "learning_rate = 2e-4\n",
    "weight_decay = 0.001\n",
    "optim = \"paged_adamw_32bit\"\n",
    "lr_scheduler_type = \"cosine\"\n",
    "max_seq_length = 2048\n",
    "warmup_ratio = 0.03\n",
    "group_by_length = True\n",
    "save_steps = 50\n",
    "save_total_limit=2\n",
    "logging_steps = 10\n",
    "max_steps = 200"
   ],
   "metadata": {
    "id": "ZWXbDLFMc2Iu"
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sft_config = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\",\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=max_seq_length,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# Print all attributes as a dictionary\n",
    "import pprint\n",
    "pprint.pprint(sft_config.to_dict())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOO6nh2Dejn4",
    "outputId": "4e548a2b-e9c4-48fc-a6ee-3ac4fef0abc8"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "PyTorch: setting up devices\n",
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'accelerator_config': {'dispatch_batches': None,\n",
      "                        'even_batches': True,\n",
      "                        'gradient_accumulation_kwargs': None,\n",
      "                        'non_blocking': False,\n",
      "                        'split_batches': False,\n",
      "                        'use_seedable_sampler': True},\n",
      " 'activation_offloading': False,\n",
      " 'adam_beta1': 0.9,\n",
      " 'adam_beta2': 0.999,\n",
      " 'adam_epsilon': 1e-08,\n",
      " 'assistant_only_loss': False,\n",
      " 'auto_find_batch_size': False,\n",
      " 'average_tokens_across_devices': True,\n",
      " 'batch_eval_metrics': False,\n",
      " 'bf16': True,\n",
      " 'bf16_full_eval': False,\n",
      " 'chat_template_path': None,\n",
      " 'completion_only_loss': None,\n",
      " 'data_seed': None,\n",
      " 'dataloader_drop_last': False,\n",
      " 'dataloader_num_workers': 0,\n",
      " 'dataloader_persistent_workers': False,\n",
      " 'dataloader_pin_memory': True,\n",
      " 'dataloader_prefetch_factor': None,\n",
      " 'dataset_kwargs': None,\n",
      " 'dataset_num_proc': None,\n",
      " 'dataset_text_field': 'text',\n",
      " 'ddp_backend': None,\n",
      " 'ddp_broadcast_buffers': None,\n",
      " 'ddp_bucket_cap_mb': None,\n",
      " 'ddp_find_unused_parameters': None,\n",
      " 'ddp_timeout': 1800,\n",
      " 'debug': [],\n",
      " 'deepspeed': None,\n",
      " 'disable_tqdm': False,\n",
      " 'do_eval': False,\n",
      " 'do_predict': False,\n",
      " 'do_train': False,\n",
      " 'enable_jit_checkpoint': False,\n",
      " 'eos_token': '<EOS_TOKEN>',\n",
      " 'eval_accumulation_steps': None,\n",
      " 'eval_delay': 0,\n",
      " 'eval_do_concat_batches': True,\n",
      " 'eval_on_start': False,\n",
      " 'eval_packing': None,\n",
      " 'eval_steps': None,\n",
      " 'eval_strategy': 'no',\n",
      " 'eval_use_gather_object': False,\n",
      " 'fp16': False,\n",
      " 'fp16_full_eval': False,\n",
      " 'fsdp': [],\n",
      " 'fsdp_config': {'min_num_params': 0,\n",
      "                 'xla': False,\n",
      "                 'xla_fsdp_grad_ckpt': False,\n",
      "                 'xla_fsdp_v2': False},\n",
      " 'full_determinism': False,\n",
      " 'gradient_accumulation_steps': 4,\n",
      " 'gradient_checkpointing': True,\n",
      " 'gradient_checkpointing_kwargs': {'use_reentrant': False},\n",
      " 'greater_is_better': None,\n",
      " 'group_by_length': True,\n",
      " 'hub_always_push': False,\n",
      " 'hub_model_id': None,\n",
      " 'hub_private_repo': None,\n",
      " 'hub_revision': None,\n",
      " 'hub_strategy': 'every_save',\n",
      " 'hub_token': '<HUB_TOKEN>',\n",
      " 'ignore_data_skip': False,\n",
      " 'include_for_metrics': [],\n",
      " 'include_num_input_tokens_seen': 'no',\n",
      " 'label_names': None,\n",
      " 'label_smoothing_factor': 0.0,\n",
      " 'learning_rate': 0.0002,\n",
      " 'length_column_name': 'length',\n",
      " 'liger_kernel_config': None,\n",
      " 'load_best_model_at_end': False,\n",
      " 'local_rank': -1,\n",
      " 'log_level': 'passive',\n",
      " 'log_level_replica': 'warning',\n",
      " 'log_on_each_node': True,\n",
      " 'logging_dir': None,\n",
      " 'logging_first_step': False,\n",
      " 'logging_nan_inf_filter': True,\n",
      " 'logging_steps': 10,\n",
      " 'logging_strategy': 'steps',\n",
      " 'loss_type': 'nll',\n",
      " 'lr_scheduler_kwargs': None,\n",
      " 'lr_scheduler_type': 'cosine',\n",
      " 'max_grad_norm': 0.3,\n",
      " 'max_length': 2048,\n",
      " 'max_steps': -1,\n",
      " 'metric_for_best_model': None,\n",
      " 'model_init_kwargs': None,\n",
      " 'neftune_noise_alpha': None,\n",
      " 'num_train_epochs': 1,\n",
      " 'optim': 'paged_adamw_32bit',\n",
      " 'optim_args': None,\n",
      " 'optim_target_modules': None,\n",
      " 'output_dir': './results',\n",
      " 'packing': False,\n",
      " 'packing_strategy': 'bfd',\n",
      " 'pad_to_multiple_of': None,\n",
      " 'pad_token': '<PAD_TOKEN>',\n",
      " 'padding_free': False,\n",
      " 'parallelism_config': None,\n",
      " 'per_device_eval_batch_size': 8,\n",
      " 'per_device_train_batch_size': 1,\n",
      " 'prediction_loss_only': False,\n",
      " 'project': 'huggingface',\n",
      " 'push_to_hub': False,\n",
      " 'remove_unused_columns': True,\n",
      " 'report_to': ['tensorboard'],\n",
      " 'restore_callback_states_from_checkpoint': False,\n",
      " 'resume_from_checkpoint': None,\n",
      " 'run_name': None,\n",
      " 'save_on_each_node': False,\n",
      " 'save_only_model': False,\n",
      " 'save_steps': 50,\n",
      " 'save_strategy': 'steps',\n",
      " 'save_total_limit': 2,\n",
      " 'seed': 42,\n",
      " 'shuffle_dataset': False,\n",
      " 'skip_memory_metrics': True,\n",
      " 'tf32': None,\n",
      " 'torch_compile': False,\n",
      " 'torch_compile_backend': None,\n",
      " 'torch_compile_mode': None,\n",
      " 'torch_empty_cache_steps': None,\n",
      " 'trackio_space_id': 'trackio',\n",
      " 'use_cache': False,\n",
      " 'use_cpu': False,\n",
      " 'use_liger_kernel': False,\n",
      " 'warmup_ratio': 0.03,\n",
      " 'warmup_steps': 0.03,\n",
      " 'weight_decay': 0.001}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Initializing SFTTrainer...\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    ")\n",
    "\n",
    "print(\"\\n Trainer initialized successfully!\")\n",
    "print(f\"  - Max sequence length: {max_seq_length}\")\n",
    "print(f\"  - Training samples: {len(dataset):,}\")\n",
    "print(f\"  - Training steps: {max_steps}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217,
     "referenced_widgets": [
      "65d423a8c50d46c59c8400f1238c949b",
      "43a25c1f63034545bf36d4220a198871",
      "ddae65e4b69748b9b57104834eb5335c",
      "88d9608f8dba4e11b4e777e9d688fceb",
      "47802dc3047c49e7a65862690d82326b",
      "008e517b2f1d4399bcf6e9b665f16982",
      "16afe083bb67486095618bd3b7e52f79",
      "494d5538a7de43c389b2ec2daf0e78a5",
      "b434f316f52b4c6289b9d7a70eb736e1",
      "8e44b7ad69cd456abcc0107b8c86f90a",
      "24808584d43f4bcbad6c00996abb861a",
      "3f1a33450e064683a69e6121d105e597",
      "e5a95b1f3d364514bbb9728814b0841a",
      "841ededcf4ea461d8dc631d2ee914540",
      "cbbcc9de8c054ede9a5fd7bf478ac144",
      "a8535f81e25e4e049bcc67486e38c5c1",
      "2ad274c4288e4d0e957ec16e7126d3e0",
      "93d0b33936e84f5cb85f18a664de4db1",
      "5b6fe31964744fcea330a92bf22d5ea2",
      "88f4126ee1a74e8e91790cbf50dfb45d",
      "5d462f7b87234b4cb4db1c7d08d5e162",
      "59964aa6f4344627a4781e303ba160bb",
      "2bca05a155e44d18b5e637a38a72094c",
      "53c5edebbffd4caf98992cc7c84d54f1",
      "91b4c563813a416d91e0b60178b83af8",
      "9bfd54697ac54a708d2a340f59dbbcfb",
      "9a7afbd090f346b4bc5f19c937f53308",
      "7ab1bbee43ec4231ba84f45fefb76d67",
      "bd077eb88678416c9b9bec11d0d14f0d",
      "cf5b1111648c4bce85daafd8462cf580",
      "637c5e8bae6e4f0090d2760decc44818",
      "303d254a8f4d4cf3b3492e4430108ab4",
      "a8b6905035d7420abf11bca461708e45"
     ]
    },
    "id": "7kwIvUCnfNPo",
    "outputId": "c2f663dc-7fb7-493b-9ff7-c8185dc48aa6"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initializing SFTTrainer...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65d423a8c50d46c59c8400f1238c949b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f1a33450e064683a69e6121d105e597"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2bca05a155e44d18b5e637a38a72094c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Trainer initialized successfully!\n",
      "  - Max sequence length: 2048\n",
      "  - Training samples: 9,846\n",
      "  - Training steps: 200\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Check trainable parameters after LoRA injection\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params:,} || \"\n",
    "        f\"all params: {all_param:,} || \"\n",
    "        f\"trainable%: {100 * trainable_params / all_param:.4f}\"\n",
    "    )\n",
    "\n",
    "print(\"=== LoRA Parameters ===\")\n",
    "print_trainable_parameters(trainer.model)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cceoDiAkfo40",
    "outputId": "5dea2b51-a1eb-4ca6-b463-1c97fa6fa846"
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== LoRA Parameters ===\n",
      "trainable params: 167,772,160 || all params: 4,708,372,480 || trainable%: 3.5633\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert normalization layers to float32 for training stability\n",
    "# This is important when using mixed precision with quantized models\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name or \"lm_head\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "print(\" Normalization layers converted to float32 for stability\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TW1l14oCfusp",
    "outputId": "a9edaf8f-28f2-4e81-9cfd-4ee2f74decc4"
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Normalization layers converted to float32 for stability\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Start training!\n",
    "print(\"=\" * 70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTraining for {max_steps} steps...\")\n",
    "print(f\"Saving checkpoints every {save_steps} steps to: {output_dir}\")\n",
    "print(\"\\nYou can monitor progress via the logging output below.\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GelXc9j2f0Xi",
    "outputId": "4ebffd9d-602c-4177-a790-155d9f5eb62a"
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================================================================\n",
      "STARTING TRAINING\n",
      "======================================================================\n",
      "\n",
      "Training for 200 steps...\n",
      "Saving checkpoints every 50 steps to: ./results\n",
      "\n",
      "You can monitor progress via the logging output below.\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The following columns in the Training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: text. If text are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 9,846\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 2,462\n",
      "  Number of trainable parameters = 167,772,160\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2462' max='2462' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2462/2462 1:54:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.452583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.397838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.496645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.610237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.742431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.266032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.191950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.392547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.502595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.698606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.151790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.269139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.385173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.517363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.737418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.251873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.226348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.178914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.597893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.639061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.283603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.292277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.168451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.473499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.693764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.210825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.284134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.451468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.644461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.709664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.072862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.334619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.308703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.583560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.580612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.268917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.226735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.263173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.509795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.742747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.265629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.278951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.332822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.629285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.729391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.132388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.237812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.350730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.491076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.780657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.388977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.223739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.284100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.441048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.779538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.227321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>1.228049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.298905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.547835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.638904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>1.209372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.303249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.302710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.481913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.724285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.243229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>1.234806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.411723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>1.557066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.714960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>1.280345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.279401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>1.331238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.597156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.707787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.200454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>1.172515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.333340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>1.611806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.779594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>1.079353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.159482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>1.390647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.532557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.829973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.235889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>1.143922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.261641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>1.469695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.660546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>1.148719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.131126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>1.299835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>1.491321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.799262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.047024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>1.200054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1.356193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>1.635150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.685823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>1.135794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>1.242568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>1.178615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.480482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.597109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>1.193514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>1.224679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.164921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>1.515209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.686635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>1.194780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>1.200644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>1.387967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>1.522057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.676592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>1.287292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>1.256806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>1.409633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>1.623096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.837215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>1.228694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>1.235188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>1.355903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>1.497917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.594114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>1.229500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>1.277564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>1.392005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>1.473245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.613724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>1.335809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>1.351155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>1.139018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>1.394145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.945131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>1.162546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>1.151059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>1.281979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>1.461153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.711682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>1.061028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>1.328448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>1.421366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>1.528843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.543384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>1.272047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>1.375958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>1.230493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>1.562663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.645003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>1.121468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>1.158169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>1.427317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>1.567043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.720006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>1.234460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>1.317979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>1.162451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>1.406502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.810999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>1.162232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>1.442168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>1.343871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>1.495096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.670987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>1.239037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>1.097961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>1.304560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>1.333576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.684921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>1.041231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>1.143656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>1.279356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>1.545978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.636877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>1.167397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>1.143528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>1.221058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>1.532374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.717292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>1.114659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>1.221621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>1.155881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>1.421031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.638654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>1.188442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>1.235462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>1.234006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>1.281047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.591561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>1.310258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>1.213454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>1.341544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>1.607037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.696551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>1.184558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>1.100930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>1.235554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>1.512347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.677783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>1.191950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>1.192129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>1.301285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>1.466315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.709909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>1.214071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>1.257637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>1.360385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>1.735826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.580009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2110</td>\n",
       "      <td>1.118878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>1.243918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>1.397445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>1.586358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.719916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>1.117954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2170</td>\n",
       "      <td>1.184340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>1.323863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2190</td>\n",
       "      <td>1.495598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.683276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2210</td>\n",
       "      <td>1.232934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>1.322353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2230</td>\n",
       "      <td>1.241618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>1.515180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.715862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>1.188092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2270</td>\n",
       "      <td>1.320544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>1.393549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2290</td>\n",
       "      <td>1.493075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.630524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>1.206515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>1.078123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>1.178834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>1.481203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>1.637674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>1.120884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2370</td>\n",
       "      <td>1.220755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>1.276703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2390</td>\n",
       "      <td>1.508596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.684198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2410</td>\n",
       "      <td>1.112187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>1.222855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2430</td>\n",
       "      <td>1.265104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>1.432539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>1.693316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>1.450461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-50\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-50/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-50/tokenizer_config.json\n",
      "Saving model checkpoint to ./results/checkpoint-100\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-100/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-100/tokenizer_config.json\n",
      "Saving model checkpoint to ./results/checkpoint-150\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-150/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-150/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-50] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-200\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-200/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-200/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-100] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-250\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-250/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-250/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-150] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-300\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-300/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-300/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-200] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-350\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-350/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-350/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-250] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-400\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-400/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-400/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-300] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-450\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-450/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-450/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-350] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-500/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-400] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-550\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-550/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-550/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-450] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-600\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-600/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-600/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-650\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-650/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-650/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-550] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-700\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-700/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-700/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-600] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-750\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-750/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-750/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-650] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-800\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-800/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-800/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-700] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-850\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-850/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-850/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-750] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-900\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-900/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-900/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-800] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-950\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-950/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-950/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-850] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-1000/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-900] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-1050\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-1050/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-1050/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-950] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-1100\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-1100/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-1100/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-1150\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-1150/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-1150/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-1050] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-1200\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-1200/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-1200/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-1100] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-1250\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-1250/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-1250/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-1150] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-1300\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-1300/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-1300/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-1200] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-1350\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-1350/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-1350/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-1250] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-1400\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-1400/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-1400/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-1300] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-1450\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-1450/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-1450/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-1350] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-1500\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-1500/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-1500/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-1400] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-1550\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-1550/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-1550/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-1450] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-1600\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-1600/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-1600/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-1650\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-1650/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-1650/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-1550] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-1700\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-1700/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-1700/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-1600] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-1750\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-1750/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-1750/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-1650] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-1800\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-1800/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-1800/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-1700] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-1850\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-1850/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-1850/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-1750] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-1900\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-1900/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-1900/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-1800] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-1950\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-1950/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-1950/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-1850] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-2000\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-2000/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-2000/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-1900] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-2050\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-2050/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-2050/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-1950] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-2100\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-2100/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-2100/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-2150\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-2150/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-2150/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-2050] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-2200\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-2200/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-2200/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-2100] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-2250\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-2250/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-2250/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-2150] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-2300\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-2300/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-2300/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-2200] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-2350\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-2350/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-2350/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-2250] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-2400\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-2400/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-2400/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-2300] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-2450\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-2450/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-2450/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-2350] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-2462\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./results/checkpoint-2462/chat_template.jinja\n",
      "tokenizer config file saved in ./results/checkpoint-2462/tokenizer_config.json\n",
      "Deleting older checkpoint [results/checkpoint-2400] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the final trained model\n",
    "final_model_path = \"./metta-llama3-8b-instruct-guanaco-lora-final\"\n",
    "\n",
    "print(f\"Saving final model to: {final_model_path}\")\n",
    "trainer.save_model(final_model_path)\n",
    "\n",
    "print(\"\\n Model saved successfully!\")\n",
    "print(f\"\\nModel location: {final_model_path}\")\n",
    "print(\"\\nYou can load this model later for inference.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MR1ygQIQf8ev",
    "outputId": "4364b04c-d424-40ce-f56f-0ef5fe2ea34f"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Saving model checkpoint to ./metta-llama3-8b-instruct-guanaco-lora-final\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving final model to: ./metta-llama3-8b-instruct-guanaco-lora-final\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_parameters\": {\n",
      "    \"rope_theta\": 500000.0,\n",
      "    \"rope_type\": \"default\"\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"5.0.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "chat template saved in ./metta-llama3-8b-instruct-guanaco-lora-final/chat_template.jinja\n",
      "tokenizer config file saved in ./metta-llama3-8b-instruct-guanaco-lora-final/tokenizer_config.json\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " Model saved successfully!\n",
      "\n",
      "Model location: ./metta-llama3-8b-instruct-guanaco-lora-final\n",
      "\n",
      "You can load this model later for inference.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "QM-tD4HrALhM"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}